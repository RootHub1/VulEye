import requests
from bs4 import BeautifulSoup
import re
import json
import time
from urllib.parse import urlparse, urljoin, parse_qs, urlencode
from collections import defaultdict, Counter
from colorama import init, Fore, Style, Back
import argparse
import base64
from pathlib import Path

init(autoreset=True)

class HiddenParamsScanner:
    def __init__(self):
        self.session = requests.Session()
        self.session.verify = False
        self.findings = []
        self.exposed_files = []
        self.wordlists_dir = Path(__file__).parent / "wordlists"
        
    def banner(self):
        print(f"{Fore.CYAN}{'‚ïê' * 90}")
        print(f"{Fore.CYAN}‚ïë{Fore.GREEN}{' ' * 24}PROFESSIONAL HIDDEN PARAMETERS DISCOVERY{Fore.GREEN}{' ' * 24}{Fore.CYAN}‚ïë")
        print(f"{Fore.CYAN}{'‚ïê' * 90}{Style.RESET_ALL}")

    def validate_target(self, target):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ü–µ–ª–∏"""
        if not target.startswith(('http://', 'https://')):
            return False, f"{Fore.RED}[!] URL –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å http(s)://{Style.RESET_ALL}"
        
        parsed = urlparse(target)
        if not parsed.netloc:
            return False, f"{Fore.RED}[!] –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL{Style.RESET_ALL}"
        return True, ""

    def load_wordlists(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä–µ–π –¥–ª—è fuzzing"""
        common_params = [
            'debug', 'admin', 'test', 'dev', 'staging', 'token', 'auth', 'session',
            'csrf', 'secret', 'key', 'password', 'api_key', 'access_token', 'id',
            'user_id', 'page', 'limit', 'offset', 'internal', 'beta', 'preview'
        ]
        
        sensitive_keywords = [
            'secret', 'token', 'password', 'key', 'pass', 'pwd', 'auth', 'session',
            'csrf', 'api_key', 'bearer', 'private', 'admin', 'root'
        ]
        
        config_files = [
            '/.env', '/.env.local', '/.env.production', '/.env.example',
            '/config.php', '/settings.php', '/debug.php', '/admin/config.php',
            '/wp-config.php', '/.htaccess', '/robots.txt', '/sitemap.xml',
            '/backup.sql', '/db.sql.gz', '/config.json', '/app.config'
        ]
        
        return common_params, sensitive_keywords, config_files

    def analyze_html_source(self, content):
        """–ê–Ω–∞–ª–∏–∑ HTML –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞"""
        findings = []
        
        
        comments = re.findall(r'<!--\s*(.*?)\s*-->', content, re.DOTALL | re.IGNORECASE)
        for comment in comments:
            if len(comment.strip()) > 5:
                risk = self.assess_risk(comment)
                findings.append({
                    'type': 'HTML Comment',
                    'value': comment.strip()[:100],
                    'risk': risk,
                    'snippet': self.truncate(comment, 80)
                })
        
        
        js_patterns = [
            r'var\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*["\']([^"\']+)["\']',
            r'let\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*["\']([^"\']+)["\']',
            r'const\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*["\']([^"\']+)["\']',
            r'window\.([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*["\']([^"\']+)["\']',
        ]
        
        for pattern in js_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for name, value in matches:
                if self.is_suspicious_param(name):
                    risk = 'CRITICAL' if self.is_sensitive_keyword(name) else 'HIGH'
                    findings.append({
                        'type': 'JS Variable',
                        'value': f"{name}={value[:50]}",
                        'risk': risk,
                        'snippet': f"{name}={value[:80]}"
                    })
        
        return findings

    def analyze_forms(self, soup):
        """–ê–Ω–∞–ª–∏–∑ —Ñ–æ—Ä–º –∏ —Å–∫—Ä—ã—Ç—ã—Ö –ø–æ–ª–µ–π"""
        findings = []
        forms = soup.find_all('form')
        
        for form in forms:
            
            hidden_inputs = form.find_all('input', {'type': 'hidden'})
            for inp in hidden_inputs:
                name = inp.get('name', '')
                value = inp.get('value', '')[:50]
                if name:
                    risk = self.assess_input_risk(name, value)
                    findings.append({
                        'type': 'Hidden Input',
                        'value': f"{name}={value}",
                        'risk': risk,
                        'snippet': f"form[{name}]={value}"
                    })
            
            
            action = form.get('action', '')
            if action:
                parsed = urlparse(action)
                for param in parse_qs(parsed.query):
                    if self.is_suspicious_param(param):
                        findings.append({
                            'type': 'Form Action Param',
                            'value': param,
                            'risk': 'MEDIUM',
                            'snippet': f"action?{param}"
                        })
        
        return findings

    def analyze_dom_attributes(self, soup):
        """–ê–Ω–∞–ª–∏–∑ DOM –∞—Ç—Ä–∏–±—É—Ç–æ–≤"""
        findings = []
        
        
        for tag in soup.find_all(attrs={'data-*': True}):
            for attr, value in tag.attrs.items():
                if attr.startswith('data-') and len(str(value)) > 8:
                    risk = 'LOW'
                    if self.is_suspicious_param(attr[5:]):  
                        risk = 'MEDIUM'
                    findings.append({
                        'type': f"Data Attribute ({attr})",
                        'value': str(value)[:50],
                        'risk': risk,
                        'snippet': f"{attr}={value[:60]}"
                    })
        
        
        suspicious_attrs = ['id', 'class']
        for tag in soup.find_all():
            for attr in suspicious_attrs:
                if attr in tag.attrs:
                    value = tag.attrs[attr]
                    if self.is_suspicious_param(value):
                        findings.append({
                            'type': f"{attr.upper()} Attribute",
                            'value': value,
                            'risk': 'MEDIUM',
                            'snippet': f"{attr}={value}"
                        })
        
        return findings

    def analyze_links_and_urls(self, soup, base_url):
        """–ê–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ –∏ URL –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        findings = []
        
        for link in soup.find_all(['a', 'link', 'script'], href=True):
            href = link['href']
            full_url = urljoin(base_url, href)
            parsed = urlparse(full_url)
            
            for param in parse_qs(parsed.query):
                if self.is_suspicious_param(param):
                    findings.append({
                        'type': 'URL Parameter',
                        'value': param,
                        'risk': 'MEDIUM',
                        'snippet': f"{full_url.split('?')[0]}?{param}=..."
                    })
        
        return findings

    def brute_force_parameters(self, base_url):
        """Fuzzing —Å–∫—Ä—ã—Ç—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        common_params, _, _ = self.load_wordlists()
        test_values = ['1', 'test', 'true', 'admin']
        
        findings = []
        base_parsed = urlparse(base_url)
        
        for param in common_params[:20]:  
            for value in test_values:
                test_url = f"{base_parsed.scheme}://{base_parsed.netloc}{base_parsed.path}?{param}={value}"
                
                try:
                    resp = self.session.get(test_url, timeout=8, allow_redirects=True)
                    
                    
                    if resp.status_code not in [404, 405] and len(resp.content) > 100:
                        findings.append({
                            'type': 'Brute Forced Param',
                            'value': f"{param}={value}",
                            'risk': 'HIGH',
                            'snippet': test_url,
                            'response_size': len(resp.content),
                            'status': resp.status_code
                        })
                    time.sleep(0.2)
                except Exception:
                    continue
        
        return findings

    def scan_config_files(self, base_url, config_files):
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        for filepath in config_files:
            test_url = urljoin(base_url, filepath)
            
            try:
                resp = self.session.get(test_url, timeout=7, allow_redirects=False)
                
                if resp.status_code == 200:
                    content_preview = resp.text[:200]
                    
                    
                    secrets_found = re.findall(r'(password|secret|key|token)[:=]\s*["\']?([^"\',\s]+)', 
                                            content_preview, re.IGNORECASE)
                    
                    risk = 'CRITICAL' if secrets_found else 'HIGH'
                    
                    self.exposed_files.append({
                        'url': test_url,
                        'size': len(resp.content),
                        'risk': risk,
                        'secrets': secrets_found,
                        'preview': content_preview[:100]
                    })
                    
            except Exception:
                continue

    def assess_risk(self, content):
        """–û—Ü–µ–Ω–∫–∞ —É—Ä–æ–≤–Ω—è —Ä–∏—Å–∫–∞"""
        sensitive_keywords = ['secret', 'token', 'password', 'key', 'pass', 'auth']
        score = sum(1 for keyword in sensitive_keywords if keyword in content.lower())
        
        if score >= 2: return 'CRITICAL'
        if score == 1: return 'HIGH'
        if len(content) > 50: return 'MEDIUM'
        return 'LOW'

    def is_suspicious_param(self, param):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞"""
        common_params, _, _ = self.load_wordlists()
        return param.lower() in [p.lower() for p in common_params]

    def is_sensitive_keyword(self, text):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        sensitive_keywords, _, _ = self.load_wordlists()
        return any(kw in text.lower() for kw in sensitive_keywords)

    def assess_input_risk(self, name, value):
        """–û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–∞ input –ø–æ–ª—è"""
        if self.is_sensitive_keyword(name):
            return 'HIGH'
        if len(value) > 20:
            return 'MEDIUM'
        return 'LOW'

    def scan(self, target):
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è"""
        self.banner()
        
        valid, error = self.validate_target(target)
        if not error:
            print(f"{Fore.RED}{error}{Style.RESET_ALL}")
            return False
        
        print(f"{Fore.GREEN}[‚úì] –°–∫–∞–Ω–∏—Ä—É–µ–º: {target}{Style.RESET_ALL}")
        
        try:
            resp = self.session.get(target, timeout=15, allow_redirects=True)
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            print(f"{Fore.GREEN}[‚úì] HTML —Ä–∞–∑–º–µ—Ä: {len(resp.content):,} –±–∞–π—Ç{Style.RESET_ALL}")
            
            
            print(f"\n{Fore.CYAN}{'=' * 90}")
            print(f"{Fore.CYAN}üîç –ù–ê–ß–ò–ù–ê–ï–ú –ú–ù–û–ì–û–£–†–û–í–ù–ï–í–´–ô –ê–ù–ê–õ–ò–ó{Style.RESET_ALL}")
            print(f"{Fore.CYAN}{'=' * 90}")
            
           
            html_findings = self.analyze_html_source(resp.text)
            self.findings.extend(html_findings)
            
            
            dom_findings = self.analyze_dom_attributes(soup)
            self.findings.extend(dom_findings)
            
           
            form_findings = self.analyze_forms(soup)
            self.findings.extend(form_findings)
            
           
            link_findings = self.analyze_links_and_urls(soup, resp.url)
            self.findings.extend(link_findings)
            
            
            print(f"\n{Fore.YELLOW}[‚è≥] Fuzzing –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...{Style.RESET_ALL}")
            fuzz_findings = self.brute_force_parameters(resp.url)
            self.findings.extend(fuzz_findings)
            
            
            print(f"{Fore.YELLOW}[‚è≥] –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥–æ–≤...{Style.RESET_ALL}")
            _, _, config_files = self.load_wordlists()
            self.scan_config_files(resp.url, config_files)
            
            self.print_report()
            return True
            
        except Exception as e:
            print(f"{Fore.RED}[!] –û—à–∏–±–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {e}{Style.RESET_ALL}")
            return False

    def truncate(self, text, length):
        """–£–º–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if len(text) <= length:
            return text
        half = length // 2
        return f"{text[:half]}...{text[-half:]}"

    def print_report(self):
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç"""
        risk_levels = Counter(f['risk'] for f in self.findings)
        
        print(f"\n{Fore.CYAN}{'‚ïê' * 90}")
        print(f"{Fore.CYAN}{' ' * 36}üìä –ü–û–õ–ù–´–ô –û–¢–ß–Å–¢{Style.RESET_ALL}")
        print(f"{Fore.CYAN}{'‚ïê' * 90}")
        
        print(f"{Fore.CYAN}–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:{Style.RESET_ALL}")
        print(f"  {Fore.RED}üî¥ –ö—Ä–∏—Ç–∏—á–Ω–æ: {risk_levels['CRITICAL']:>2}  {Fore.RED}üü° –í—ã—Å–æ–∫–∏–π: {risk_levels['HIGH']:>2}")
        print(f"  {Fore.YELLOW}üü† –°—Ä–µ–¥–Ω–∏–π: {risk_levels['MEDIUM']:>2}  {Fore.GREEN}üü¢ –ù–∏–∑–∫–∏–π: {risk_levels['LOW']:>2}")
        print(f"  üìä –í—Å–µ–≥–æ: {len(self.findings):>2} | üìÅ –ö–æ–Ω—Ñ–∏–≥–∏: {len(self.exposed_files):>2}")
        
        print(f"\n{Fore.MAGENTA}{'‚ïê' * 90}")
        print(f"{Fore.MAGENTA}üéØ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ù–ê–•–û–î–ö–ò (CRITICAL/HIGH){Style.RESET_ALL}")
        print(f"{Fore.MAGENTA}{'‚ïê' * 90}")
        
        critical_findings = [f for f in self.findings if f['risk'] in ['CRITICAL', 'HIGH']]
        for i, finding in enumerate(critical_findings, 1):
            marker = Fore.RED + "üö® CRITICAL" + Style.RESET_ALL if finding['risk'] == 'CRITICAL' else Fore.MAGENTA + "‚ö†Ô∏è HIGH" + Style.RESET_ALL
            print(f"{Fore.CYAN}{i:2d}.{Style.RESET_ALL} {marker} {finding['type']:<20} {finding['value']}")
        
        if self.exposed_files:
            print(f"\n{Fore.RED}{'‚ïê' * 90}")
            print(f"{Fore.RED}üíæ –û–¢–ö–†–´–¢–´–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–û–ù–ù–´–ï –§–ê–ô–õ–´{Style.RESET_ALL}")
            print(f"{Fore.RED}{'‚ïê' * 90}")
            for file in self.exposed_files:
                print(f"{Fore.RED}[!] {file['url']} ({file['size']:,}B) - {file['risk']}{Style.RESET_ALL}")
                if file.get('secrets'):
                    print(f"     Secrets: {file['secrets']}")


def run():
    '''Wrapper function for main.py integration'''
    try:
        main()
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print(f"Error: {e}")


def main():
    parser = argparse.ArgumentParser(description='–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–∫–∞–Ω–µ—Ä —Å–∫—Ä—ã—Ç—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤')
    parser.add_argument('target', nargs='?', help='–¶–µ–ª–µ–≤–æ–π URL')
    parser.add_argument('-u', '--url', dest='target', help='–¶–µ–ª–µ–≤–æ–π URL')
    args = parser.parse_args()
    
    target = args.target
    if not target:
        target = input(f"{Fore.YELLOW}[+] –í–≤–µ–¥–∏—Ç–µ URL: {Style.RESET_ALL}").strip()
    
    scanner = HiddenParamsScanner()
    scanner.scan(target)

if __name__ == "__main__":
    requests.packages.urllib3.disable_warnings()
    main()